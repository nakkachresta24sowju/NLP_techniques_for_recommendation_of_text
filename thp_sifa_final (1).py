# -*- coding: utf-8 -*-
"""THP_SIFA_Final

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MySC5lFD5l19WiyeAtZVPSlbBRP2oogx

Based on the dataset provided below, youâ€™re required to develop a model to generate a short summary that accurately summarizes the main points and key insights (in negative and positive content). The summary should be concise, yet still, retain the essence of the original text. The summarization model should generate personalized summaries based on a set of user-specific keywords. Then, extend the summarization model to provide additional suggestions or recommendations based on the contents.

Import the Neccessary Libraries
"""

!pip install streamlit
!pip install ipython==7.9.0
!pip install pyngrok

import streamlit as st
import numpy as np
import pandas as pd
from tqdm import tqdm
import re

import warnings
warnings.filterwarnings("ignore")
import pickle

from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem.porter import PorterStemmer
from nltk import pos_tag
from nltk import ne_chunk

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics 
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

import nltk
nltk.download('punkt') 
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
nltk.download('vader_lexicon')

!pip install pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile text_analytics_app.py 
# #Title of the project
# st.title("Text summarization that will recommend suggestions based on text analytics and sentiment analysis")

from pyngrok import ngrok 
public_url = ngrok.connect(port='8501')
public_url

!streamlit run /content/text_analytics_app.py & npx localtunnel --port 8501

"""Read the dataset"""

from google.colab import drive
drive.mount('/content/drive')

#Read the data
df = pd.read_csv('drive/My Drive/SIFA_DS_Intern/FF_dataset.csv')

df.head()

df.rename(columns = {'email summary':'email_summary'}, inplace = True)

df['notes'][3]

df['email_summary'][3]

"""Droping Duplicates and NA values"""

df.drop_duplicates(subset=['notes'],inplace=True)  #dropping duplicates
df.drop_duplicates(subset=['email_summary'],inplace=True)   #dropping duplicates
df.dropna(axis=0,inplace=True)  #dropping nan

"""Basic information about the dataset"""

df.info()

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove*.zip

"""Word embeddings for glove vector"""

# Extract word vectors
word_embeddings = {}
f = open('glove.6B.100d.txt', encoding='utf-8')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    word_embeddings[word] = coefs
f.close()

len(word_embeddings)

"""Pre-processing Text data (Email_Summary)"""

def decontracted(phrase):
    # specific
    phrase = re.sub(r"won't", "will not", phrase)
    phrase = re.sub(r"can\'t", "can not", phrase)

    # general
    phrase = re.sub(r"n\'t", " not", phrase)
    phrase = re.sub(r"\'re", " are", phrase)
    phrase = re.sub(r"\'s", " is", phrase)
    phrase = re.sub(r"\'d", " would", phrase)
    phrase = re.sub(r"\'ll", " will", phrase)
    phrase = re.sub(r"\'t", " not", phrase)
    phrase = re.sub(r"\'ve", " have", phrase)
    phrase = re.sub(r"\'m", " am", phrase)
    return phrase

text = df['email_summary'][6]

def preprocessed_text(text):
  
  text = text.lower()
  text=re.sub('[\w\.-]+@[\w\.-]+\.\w+', ' ',text)
  text= re.sub("[\(\[].*?[\)\]]", "", text)
  text=re.sub("Subject:.*\w+",'',text)

  #3. Delete all the sentances where sentence starts with "Write to:" or "From:".
  text=re.sub("From:.*?", ' ',text)
  text=re.sub("Write to:.*?",' ',text)

  # 4. Delete all the tags like "< anyword >"
  clean = re.compile('<.*?>')
  text=re.sub(clean,' ',text)

  # 5. Delete all the data which are present in the brackets. 
  clean1 = re.compile('\(.*\)')
  text=re.sub(clean1,'',text)

  #6. Remove all the newlines('\n'), tabs('\t'), "-", "\".
  text= re.sub(r"[\n\t-]*", "", text)

  #Remove all the words which ends with ":".
  text= re.sub(r'\w+:\s?',' ',text)

  text= re.sub('[^A-Za-z0-9]+', ' ',text)

  text = ' '.join(e.lower() for e in text.split(' '))
  text = ' '.join(e for e in text.split(' ')  if len(e)>2 and len(e)<15)

  #Decontraction of text
  text = decontracted(text)

  return text

preprocessed_text(text)

preprocessed_email_summary=[]

for i in tqdm(range(df.shape[0])):
  preprocessed_email_summary.append(preprocessed_text(df['email_summary'].values[i]))

preprocessed_email_summary[6]

"""Preprocessing text for notes"""

def preprocessed_notes_text(text):
  
  text = text.lower()
  text=re.sub('[\w\.-]+@[\w\.-]+\.\w+', ' ',text)
  text= re.sub("[\(\[].*?[\)\]]", "", text)
  text=re.sub("Subject:.*\w+",'',text)

  #3. Delete all the sentances where sentence starts with "Write to:" or "From:".
  text=re.sub("From:.*?", ' ',text)
  text=re.sub("Write to:.*?",' ',text)

  # 4. Delete all the tags like "< anyword >"
  clean = re.compile('<.*?>')
  text=re.sub(clean,' ',text)

  # 5. Delete all the data which are present in the brackets. 
  clean1 = re.compile('\(.*\)')
  text=re.sub(clean1,'',text)

  #6. Remove all the newlines('\n'), tabs('\t'), "-", "\".
  text= re.sub(r"[\n\t-]*", "", text)

  #Remove all the words which ends with ":".
  text= re.sub(r'\w+:\s?',' ',text)

  #text= re.sub('[^A-Za-z0-9]+', ' ',text)

  #text = ' '.join(e.lower() for e in text.split(' '))
  #text = ' '.join(e for e in text.split(' ')  if len(e)>2 and len(e)<15)

  #Decontraction of text
  text = decontracted(text)

  return text

preprocessed_notes = []

for i in tqdm(range(df.shape[0])):
  preprocessed_notes.append(preprocessed_notes_text(df['notes'].values[i]))

preprocessed_notes[6]

from nltk.corpus import stopwords
stop_words = stopwords.words('english')

# function to remove stopwords
def remove_stopwords(sen):
  sen_new = " ".join([i for i in sen if i not in stop_words])
  return sen_new

# remove stopwords from the sentences
clean_sentences_email = [remove_stopwords(r.split()) for r in preprocessed_email_summary]
clean_sentences_notes = [remove_stopwords(r.split()) for r in preprocessed_notes]

clean_sentences_notes[6]

clean_sentences_email[6]

df['preprocessed_email_summary'] = clean_sentences_email
df['preprocessed_notes'] = clean_sentences_notes

df.head(1)

"""**Text** **Analytics**

Sentence Tokenization
"""

from nltk.tokenize import sent_tokenize
tokenized_email_text= []
tokenized_email_word = []
tokenized_notes_text = []
tokenized_notes_word = []

fdist_email = []
fdist_notes = []

for i in tqdm(range(df.shape[0])):

  #Sentence Tokenization
  tokenized_email_text.append(sent_tokenize(df['preprocessed_email_summary'].values[i]))

  #Word Tokenization
  tokenized_email_word.append(word_tokenize(df['preprocessed_email_summary'].values[i]))


  #Sentence Tokenization
  tokenized_notes_text.append(sent_tokenize(df['preprocessed_notes'].values[i]))

  #Word Tokenization
  tokenized_notes_word.append(word_tokenize(df['preprocessed_notes'].values[i]))

#Frequency distribution table
for each_word in tokenized_email_word:
  fdist_email = FreqDist(each_word)

for each_word in tokenized_notes_word:
  fdist_notes = FreqDist(each_word)

fdist_email

fdist_notes

# Frequency Distribution Plot
import matplotlib.pyplot as plt

fdist_email.plot(30,cumulative=False)
plt.show()

# Frequency Distribution Plot
import matplotlib.pyplot as plt

fdist_notes.plot(30,cumulative=False)
plt.show()

"""Removing Stopwords from email_text and notes_text

"""

filtered_tokens_email=[]
for w in tokenized_email_word:    
    if w not in stop_words:
         filtered_tokens_email.append(w)

print("Tokenized Words:",tokenized_email_word)
print("Filterd Tokens:",filtered_tokens_email)

filtered_tokens_notes=[]
for w in tokenized_notes_word:    
    if w not in stop_words:
         filtered_tokens_notes.append(w)

print("Tokenized Words:",tokenized_notes_word)
print("Filterd Tokens:",filtered_tokens_notes)

import string

# removing punctuations
punctuations=list(string.punctuation)

filtered_tokens_notes2=[]
filtered_tokens_email2=[]

for i in filtered_tokens_notes:
    if i not in punctuations:
        filtered_tokens_notes2.append(i)

for i in filtered_tokens_email:
    if i not in punctuations:
        filtered_tokens_email2.append(i)
        
print("Filterd Tokens After Removing Punctuations:",filtered_tokens_notes2)
print("Filterd Tokens After Removing Punctuations:",filtered_tokens_email2)

"""Weighted word frequency"""

maximum_frequncy = max(fdist_email.values())

for word in fdist_email.keys():
    fdist_email[word] = (fdist_email[word]/maximum_frequncy)

maximum_frequncy

df['email_summary'][4]

sent_score_list = []

for each_sent in df['email_summary']:
  sentence_scores = {}
  for sentence in nltk.sent_tokenize(each_sent):
    for word in nltk.word_tokenize(sentence):
        if word in fdist_email and len(sentence.split(' ')) < 30:
            if sentence not in sentence_scores:
              sentence_scores[sentence] = fdist_email[word]
            else:
              sentence_scores[sentence] += fdist_email[word]

  sent_score_list.append(sentence_scores)

len(sent_score_list)

sent_score_list[4]

import heapq

sentence_summary = []
for sent_score in sent_score_list:
  summary_sentences = heapq.nlargest(4, sent_score, key=sent_score.get)
  summary = ' '.join(summary_sentences)
  sentence_summary.append(summary)

print(len(sentence_summary))

!pip install rouge-score

from rouge_score import rouge_scorer
scores = []
for i in tqdm(range(df.shape[0])):
  scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
  scores.append(scorer.score(df['email_summary'].values[i],sentence_summary[i]))

scores

df['ets_nltk_email_summary'] = sentence_summary

df.head(2)

"""Gensim implementation of TextRank extractive summarization algorithm



"""

import gensim
from gensim.summarization import summarize

def text_rank(text):
  # Summarization when both ratio & word count is given
  summary=summarize(text, ratio = 0.2)
  return summary

txtrank_email_summary=[]

for i in tqdm(range(df.shape[0])):
  txtrank_email_summary.append(text_rank(df['email_summary'].values[i]))

df['txtrank_email_summary'] = txtrank_email_summary

df.head(2)

from rouge_score import rouge_scorer
scores = []
for i in tqdm(range(df.shape[0])):
  scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
  scores.append(scorer.score(df['email_summary'].values[i], df['txtrank_email_summary'].values[i]))

scores

"""Understanding the distribution of the sequences
Here, we will analyze the length of the reviews and the summary to get an overall idea about the distribution of length of the text. This will help us fix the maximum length of the sequence:
"""

import matplotlib.pyplot as plt
notes_word_count = []
summary_word_count = []

# populate the lists with sentence lengths
for i in df['notes']:
      notes_word_count.append(len(i.split()))

for i in df['email_summary']:
      summary_word_count.append(len(i.split()))

length_df = pd.DataFrame({'notes':notes_word_count, 'email_summary':summary_word_count})
length_df.hist(bins = 30)
plt.show()

"""**Sentiment** **Analysis**"""

df['total_len'] = df['preprocessed_notes'].map(lambda x: len(x))

from nltk.sentiment.vader import SentimentIntensityAnalyzer
sentiment = SentimentIntensityAnalyzer()
df['polarity'] = df['preprocessed_notes'].map(lambda x: sentiment.polarity_scores(x))

df = pd.concat([df.drop(['polarity'], axis=1), df['polarity'].apply(pd.Series)], axis=1)
df.head(1)

df[["total_len"]].describe()

"""**Based on sentiment score make classification**"""

df['sentiment'] = df['compound'].apply(lambda x: 'positive' if x >0 else 'neutral' if x==0 else 'negative')
df.head(1)

import seaborn as sns
sns.countplot(y='sentiment', data=df, palette=['#b2d8d8',"#008080", '#db3d13']);

# Boxplot
sns.boxplot(y='compound', 
            x='sentiment',
            palette=['#b2d8d8',"#008080", '#db3d13'], 
            data=df);

# Obtain top 10 words
top_10 = fdist_email.most_common(10)

# Create pandas series to make plotting easier
fdist = pd.Series(dict(top_10))
import seaborn as sns
sns.set_theme(style="ticks")

sns.barplot(y=fdist.index, x=fdist.values, color='blue');

# Obtain top 10 words
top_10 = fdist_notes.most_common(10)

# Create pandas series to make plotting easier
fdist = pd.Series(dict(top_10))
import seaborn as sns
sns.set_theme(style="ticks")

sns.barplot(y=fdist.index, x=fdist.values, color='blue');

"""**Text** **analytics**

"""

# Let's count the number of tweets by sentiments
sentiment_counts = df.groupby(['sentiment']).size()
print(sentiment_counts)

# Let's visualize the sentiments
fig = plt.figure(figsize=(6,6), dpi=100)
ax = plt.subplot(111)
sentiment_counts.plot.pie(ax=ax, autopct='%1.1f%%', startangle=270, fontsize=12, label="")

"""Named Entity Recognition for both notes and email summary

"""

from nltk import ne_chunk

for i in tqdm(range(df.shape[0])):
  for chunk in ne_chunk(nltk.pos_tag(word_tokenize(df['notes'].values[i]))):
        if hasattr(chunk, 'label'):
            print(chunk.label(), ' '.join(c[0] for c in chunk))

for i in tqdm(range(df.shape[0])):
  for chunk in ne_chunk(nltk.pos_tag(word_tokenize(df['email_summary'].values[i]))):
        if hasattr(chunk, 'label'):
          print(chunk.label(), ' '.join(c[0] for c in chunk))

!pip install transformers

from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")
model = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")

example = "My name is yugesh and I live in India"

NER = pipeline("ner", model=model, tokenizer=tokenizer)
results = NER(df['notes'][2])
results

"""Key Word Extraction"""

!pip3 install keybert

from keybert import KeyBERT
kw_model = KeyBERT()
keywords = kw_model.extract_keywords(df['notes'][10])
print(keywords)

"""**Recommendation engine based on Named Entity Recognition**"""

